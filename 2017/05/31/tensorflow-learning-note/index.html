<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />


















  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="深度学习,TensorFlow," />





  <link rel="alternate" href="/atom.xml" title="小土刀" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="本文是我学习深度学习的笔记，来自网上的各类 jupyter notebook。">
<meta property="og:type" content="article">
<meta property="og:title" content="【不周山之数据科学】TensorFlow 学习笔记">
<meta property="og:url" content="http://wdxtub.com/2017/05/31/tensorflow-learning-note/index.html">
<meta property="og:site_name" content="小土刀">
<meta property="og:description" content="本文是我学习深度学习的笔记，来自网上的各类 jupyter notebook。">
<meta property="og:image" content="http://wdxtub.com/misc/100offer.jpg">
<meta property="og:updated_time" content="2017-07-08T23:38:16.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="【不周山之数据科学】TensorFlow 学习笔记">
<meta name="twitter:description" content="本文是我学习深度学习的笔记，来自网上的各类 jupyter notebook。">
<meta name="twitter:image" content="http://wdxtub.com/misc/100offer.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: false,
    motion: true,
    duoshuo: {
      userId: 'undefined',
      author: '小土刀'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://wdxtub.com/2017/05/31/tensorflow-learning-note/"/>





  <title> 【不周山之数据科学】TensorFlow 学习笔记 | 小土刀 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  







  <script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=59042340";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>



<script>
  	var _mtac = {};
  	(function() {
  		var mta = document.createElement("script");
  		mta.src = "http://pingjs.qq.com/h5/stats.js?v2.0.2";
  		mta.setAttribute("name", "MTAH5");
  		mta.setAttribute("sid", "500421623");

  		var s = document.getElementsByTagName("script")[0];
  		s.parentNode.insertBefore(mta, s);
  	})();
</script>






  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">小土刀</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">Agony is my triumph</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-work">
          <a href="/2016/09/11/work-page" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-wifi"></i> <br />
            
            不周山
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tech">
          <a href="/2009/09/11/tech-page" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-gear"></i> <br />
            
            通天塔
          </a>
        </li>
      
        
        <li class="menu-item menu-item-life">
          <a href="/1990/09/11/life-page" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-compass"></i> <br />
            
            好望角
          </a>
        </li>
      
        
        <li class="menu-item menu-item-booklist">
          <a href="/1997/09/11/booklist-page" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-diamond"></i> <br />
            
            书影音
          </a>
        </li>
      
        
        <li class="menu-item menu-item-thanks">
          <a href="/thanks" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://wdxtub.com/2017/05/31/tensorflow-learning-note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wdxtub">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/misc/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小土刀">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
            
            
              
                【不周山之数据科学】TensorFlow 学习笔记
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-31T17:05:32+08:00">
                2017-05-31
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2017-07-09T07:38:16+08:00">
                2017-07-09
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Technique/" itemprop="url" rel="index">
                    <span itemprop="name">Technique</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a class="cloud-tie-join-count" href="/2017/05/31/tensorflow-learning-note/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count join-count" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          
            <div class="post-wordcount">
              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                  
                    <span title="Words count in article" }}">
                      5,852
                    </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                  
                    <span title="Reading time" }}">
                      27
                    </span>
              
            </div>
          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文是我学习深度学习的笔记，来自网上的各类 jupyter notebook。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2017.05.31: 开始更新</li>
</ul>
<h2 id="快速入门"><a href="#快速入门" class="headerlink" title="快速入门"></a>快速入门</h2><p>TensorFlow 是一个基于图计算的通用系统，常常被用于进行机器学习等任务。</p>
<p>TensorFlow 由<strong>张量</strong>(tensor)而得名，张量是多维的数组。一个向量是一维数组，我们称为一阶张量；一个矩阵是二维数组，我们称为二阶张量。名字中的 flow 表示计算的过程是基于图的，数据在图中流动。神经网络的训练和推断包含计算图中许多节点的矩阵计算的传播(propagation)</p>
<p>在 TensorFlow 中搞事的流程大概是：创建张量 -&gt; 添加计算操作 -&gt; 执行。很重要的一点是定义这些操作时，计算并不会立即执行，TensorFlow 在所有的操作添加完成后，会优化计算图，决定如何计算，最后才生成各种数据。正因如此，TensorFlow 中的 tensor 可以看作是一个占位符，等数据到来，然后执行计算。</p>
<p>我们来看看如何用 TensorFlow 做向量相加，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="comment"># TensorFlow 中的操作都需要在 Session 的上下文中进行，Session 本身保存计算图的信息（张量和操作）</span></div><div class="line"><span class="keyword">with</span> tf.Session():</div><div class="line">    input1 = tf.constant([<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>])</div><div class="line">    input2 = tf.constant(<span class="number">2.0</span>, shape=[<span class="number">4</span>])</div><div class="line">    input3 = tf.constant(<span class="number">3.0</span>, shape=[<span class="number">4</span>])</div><div class="line">    output = tf.add(tf.add(input1, input2), input3)</div><div class="line">    <span class="comment"># 具体的计算在执行这一句时进行，因为 result 的值需要计算才可以得到，前面都是在定义计算图</span></div><div class="line">    result = output.eval()</div><div class="line">    print(<span class="string">"result: "</span>, result)</div></pre></td></tr></table></figure>
<p>处理完向量，我们来看看矩阵，其实也差不多</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># 矩阵相加</span></div><div class="line"><span class="keyword">with</span> tf.Session():</div><div class="line">    input1 = tf.constant(<span class="number">1.0</span>, shape=[<span class="number">2</span>, <span class="number">3</span>])</div><div class="line">    input2 = tf.constant(np.reshape(np.arange(<span class="number">1.0</span>, <span class="number">7.0</span>, dtype=np.float32), (<span class="number">2</span>, <span class="number">3</span>)))</div><div class="line">    output = tf.add(input1, input2)</div><div class="line">    print(output.eval())</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session():</div><div class="line">    input_features = tf.constant(np.reshape([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], (<span class="number">1</span>, <span class="number">4</span>)).astype(np.float32))</div><div class="line">    weights = tf.constant(np.random.randn(<span class="number">4</span>, <span class="number">2</span>).astype(np.float32))</div><div class="line">    output = tf.matmul(input_features, weights)</div><div class="line">    print(<span class="string">"Input:"</span>)</div><div class="line">    print(input_features.eval())</div><div class="line">    print(<span class="string">"Weights:"</span>)</div><div class="line">    print(weights.eval())</div><div class="line">    print(<span class="string">"Output:"</span>)</div><div class="line">    print(output.eval())</div></pre></td></tr></table></figure>
<p>前面我们都是用的 constant 常量，接下来我们搞一搞变量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment">#@test &#123;"output": "ignore"&#125;</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># 设置俩变量 total 和 weights，会不停变化</span></div><div class="line">    total = tf.Variable(tf.zeros([<span class="number">1</span>, <span class="number">2</span>]))</div><div class="line">    weights = tf.Variable(tf.random_uniform([<span class="number">1</span>,<span class="number">2</span>]))</div><div class="line">    </div><div class="line">    <span class="comment"># 初始化刚才定义的变量</span></div><div class="line">    tf.global_variables_initializer().run()</div><div class="line">    </div><div class="line">    <span class="comment"># 更新数值，但是这里并不会真的计算</span></div><div class="line">    update_weights = tf.assign(weigths, tf.random_uniform([<span class="number">1</span>, <span class="number">2</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>))</div><div class="line">    update_total = tf.assign(total, tf.add(total, weights))</div><div class="line">    </div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</div><div class="line">        <span class="comment"># 这里要先更新 weights 再更新 total</span></div><div class="line">        sess.run(update_weights)</div><div class="line">        sess.run(update_total)</div><div class="line">        </div><div class="line">        print(weights.eval(), total.eval())</div></pre></td></tr></table></figure>
<p>这里我们注意是先建模再计算这个流程即可。</p>
<h2 id="一个简单的神经网络"><a href="#一个简单的神经网络" class="headerlink" title="一个简单的神经网络"></a>一个简单的神经网络</h2><p>我们来构造一个非常简单的神经网络，来计算 x 和 y 这两个变量的线性回归。这个函数会为我们随机生成的带噪声的线性数据找到最合适的  $w_1$ 和 $w_2$，即满足 $y = w_2x+w_1$ 。代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 设置带噪声的线性数据</span></div><div class="line">num_examples = <span class="number">50</span></div><div class="line"><span class="comment"># 这里会生成一个完全线性的数据</span></div><div class="line">X = np.array([np.linspace(<span class="number">-2</span>, <span class="number">4</span>, num_examples), np.linspace(<span class="number">-6</span>, <span class="number">6</span>, num_examples)])</div><div class="line"><span class="comment"># 数据展示</span></div><div class="line"><span class="comment"># plt.figure(figsize=(4,4))</span></div><div class="line"><span class="comment"># plt.scatter(X[0], X[1])</span></div><div class="line"><span class="comment"># plt.show</span></div><div class="line"></div><div class="line"><span class="comment"># 这里给数据增加噪声</span></div><div class="line">X += np.random.randn(<span class="number">2</span>, num_examples)</div><div class="line"><span class="comment"># 数据展示</span></div><div class="line"><span class="comment"># plt.figure(figsize=(4,4))</span></div><div class="line"><span class="comment"># plt.scatter(X[0], X[1])</span></div><div class="line"><span class="comment"># plt.show</span></div><div class="line"></div><div class="line"><span class="comment"># 我们的目标就是通过学习，找到一条拟合曲线，去还原最初的线性数据</span></div><div class="line"><span class="comment"># 把数据分离成 x 和 y</span></div><div class="line">x, y = X</div><div class="line"><span class="comment"># 添加固定为 1 的 bias</span></div><div class="line">x_with_bias = np.array([(<span class="number">1.</span>, a) <span class="keyword">for</span> a <span class="keyword">in</span> x]).astype(np.float32)</div><div class="line"></div><div class="line"><span class="comment"># 用来记录每次迭代的 loss，之后用于展示结果</span></div><div class="line">losses = []</div><div class="line"><span class="comment"># 迭代次数</span></div><div class="line">training_steps = <span class="number">50</span></div><div class="line"><span class="comment"># 学习率，也叫做步长，表示我们在梯度下降时每次迭代所前进的长度，过大则学不到准确的值，过小则训练太慢</span></div><div class="line">learning_rate = <span class="number">0.002</span></div><div class="line"></div><div class="line"><span class="comment"># TensorFlow 中所有的代码都需要在 session 中</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># 设置所有的张量，变量和操作</span></div><div class="line">    <span class="comment"># 输入层是 x 值和 bias 节点</span></div><div class="line">    input = tf.constant(x_with_bias)</div><div class="line">    <span class="comment"># target 是 y 的值，需要被调整成正确的尺寸（就是转置一下）</span></div><div class="line">    target = tf.constant(np.transpose([y]).astype(np.float32))</div><div class="line">    <span class="comment"># weights 是变量，每次循环都会变，这里直接随机初始化（高斯分布，均值 0，标准差 0.1）</span></div><div class="line">    weights = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">1</span>], <span class="number">0</span>, <span class="number">0.1</span>))</div><div class="line">    </div><div class="line">    <span class="comment"># 初始化所有的变量</span></div><div class="line">    tf.global_variables_initializer().run()</div><div class="line">    </div><div class="line">    <span class="comment"># 设置循环中所要做的全部操作</span></div><div class="line">    <span class="comment"># 对于所有的 x，根据现有的 weights 来产生对应的 y 值，也就是计算 y = w2 * x + w1 * bias</span></div><div class="line">    yhat = tf.matmul(input, weights)</div><div class="line">    <span class="comment"># 计算误差，也就是预计的 y 和真实的 y 的区别</span></div><div class="line">    yerror = tf.subtract(yhat, target)</div><div class="line">    <span class="comment"># 我们想要最小化 L2 损失，是误差的平方，会惩罚大误差，放过小误差</span></div><div class="line">    loss = tf.nn.l2_loss(yerror)</div><div class="line">    <span class="comment"># 上面的 loss 函数相当于</span></div><div class="line">    <span class="comment"># loss = 0.5 * tf.reduce_sum(tf.multiply(yerror, yerror))</span></div><div class="line">    </div><div class="line">    <span class="comment"># 执行梯度下降</span></div><div class="line">    <span class="comment"># 更新 weights，比如 weights += grads * learning_rate</span></div><div class="line">    <span class="comment"># 使用偏微分更新 weights</span></div><div class="line">    update_weights = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</div><div class="line">    <span class="comment"># 上面的梯度下降相当于</span></div><div class="line">    <span class="comment"># gradient = tf.reduce_sum(tf.transpose(tf.multiply(input, yerror)), 1, keep_dims=True)</span></div><div class="line">    <span class="comment"># update_weights = tf.assign_sub(weights, learning_rate * gradient)</span></div><div class="line">    </div><div class="line">    <span class="comment"># 现在我们定义了所有的张量，也初始化了所有操作（每次执行梯度下降优化）</span></div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(training_steps):</div><div class="line">        <span class="comment"># 重复跑，更新变量</span></div><div class="line">        update_weights.run()</div><div class="line">        <span class="comment"># 如果没有用 tf.train.GradientDescentOptimizer，就要用下面的方式</span></div><div class="line">        <span class="comment"># sess.run(update_weights)</span></div><div class="line">        </div><div class="line">        <span class="comment"># 记录每次迭代的 loss</span></div><div class="line">        losses.append(loss.eval())</div><div class="line">    </div><div class="line">    <span class="comment"># 训练结束</span></div><div class="line">    betas = weights.eval()</div><div class="line">    yhat = yhat.eval()</div><div class="line"></div><div class="line"><span class="comment"># 展示训练趋势</span></div><div class="line">fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line">plt.subplots_adjust(wspace=<span class="number">.3</span>)</div><div class="line">fig.set_size_inches(<span class="number">10</span>, <span class="number">4</span>)</div><div class="line">ax1.scatter(x, y, alpha=<span class="number">.7</span>)</div><div class="line">ax1.scatter(x, np.transpose(yhat)[<span class="number">0</span>], c=<span class="string">"g"</span>, alpha=<span class="number">.6</span>)</div><div class="line">line_x_range = (<span class="number">-4</span>, <span class="number">6</span>)</div><div class="line">ax1.plot(line_x_range, [betas[<span class="number">0</span>] + a * betas[<span class="number">1</span>] <span class="keyword">for</span> a <span class="keyword">in</span> line_x_range], <span class="string">"g"</span>, alpha=<span class="number">.6</span>)</div><div class="line">ax2.plot(range(<span class="number">0</span>, training_steps), losses)</div><div class="line">ax2.set_ylabel(<span class="string">"Loss"</span>)</div><div class="line">ax2.set_xlabel(<span class="string">"Training steps"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<h2 id="从零开始上手-MNIST-数据集"><a href="#从零开始上手-MNIST-数据集" class="headerlink" title="从零开始上手 MNIST 数据集"></a>从零开始上手 MNIST 数据集</h2><p>这是 Tensorflow 官方 Docker 镜像中的最后一篇教程，主要介绍如何利用 tf 完成手写数字的识别，这里我把 jupyter 的代码稍加改动（并加了些注释），方便大家在本地使用。</p>
<p>注：详细的解释都在注释里了，这里就不再赘述。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> gzip, binascii, struct, numpy</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">from</span> six.moves.urllib.request <span class="keyword">import</span> urlretrieve</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="comment"># 这里需要翻墙，不然下载巨慢</span></div><div class="line">SOURCE_URL = <span class="string">'http://yann.lecun.com/exdb/mnist/'</span></div><div class="line">WORK_DIRECTORY = <span class="string">"./mnist-data"</span></div><div class="line"></div><div class="line"><span class="comment"># 如果下载好了，那么就不会再次下载</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybe_download</span><span class="params">(filename)</span>:</span></div><div class="line">    <span class="string">"""A helper to download the data files if not present."""</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(WORK_DIRECTORY):</div><div class="line">        os.mkdir(WORK_DIRECTORY)</div><div class="line">    filepath = os.path.join(WORK_DIRECTORY, filename)</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(filepath):</div><div class="line">        filepath, _ = urlretrieve(SOURCE_URL + filename, filepath)</div><div class="line">        statinfo = os.stat(filepath)</div><div class="line">        print(<span class="string">'Successfully downloaded'</span>, filename, statinfo.st_size, <span class="string">'bytes.'</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        print(<span class="string">'Already downloaded'</span>, filename)</div><div class="line">    <span class="keyword">return</span> filepath</div><div class="line"></div><div class="line"><span class="comment"># 这里把所有的训练数据都搞下来</span></div><div class="line">train_data_filename = maybe_download(<span class="string">'train-images-idx3-ubyte.gz'</span>)</div><div class="line">train_labels_filename = maybe_download(<span class="string">'train-labels-idx1-ubyte.gz'</span>)</div><div class="line">test_data_filename = maybe_download(<span class="string">'t10k-images-idx3-ubyte.gz'</span>)</div><div class="line">test_labels_filename = maybe_download(<span class="string">'t10k-labels-idx1-ubyte.gz'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 如果遇到 MacOS 的 Python as a framework 的问题，参考</span></div><div class="line"><span class="comment"># https://stackoverflow.com/questions/29433824/unable-to-import-matplotlib-pyplot-as-plt-in-virtualenv</span></div><div class="line"><span class="comment"># 这里我们先看看数据集里有什么，只是一个展示，并不会对图片进行预处理</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sanity_check</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">with</span> gzip.open(test_data_filename) <span class="keyword">as</span> f:</div><div class="line">        <span class="comment"># Print the header fields.</span></div><div class="line">        <span class="keyword">for</span> field <span class="keyword">in</span> [<span class="string">'magic number'</span>, <span class="string">'image count'</span>, <span class="string">'rows'</span>, <span class="string">'columns'</span>]:</div><div class="line">            <span class="comment"># struct.unpack reads the binary data provided by f.read.</span></div><div class="line">            <span class="comment"># The format string '&gt;i' decodes a big-endian integer, which</span></div><div class="line">            <span class="comment"># is the encoding of the data.</span></div><div class="line">            print(field, struct.unpack(<span class="string">'&gt;i'</span>, f.read(<span class="number">4</span>))[<span class="number">0</span>])</div><div class="line">        </div><div class="line">        <span class="comment"># Read the first 28x28 set of pixel values. </span></div><div class="line">        <span class="comment"># Each pixel is one byte, [0, 255], a uint8.</span></div><div class="line">        buf = f.read(<span class="number">28</span> * <span class="number">28</span>)</div><div class="line">        image = numpy.frombuffer(buf, dtype=numpy.uint8)</div><div class="line">    </div><div class="line">        <span class="comment"># Print the first few values of image.</span></div><div class="line">        print(<span class="string">'First 10 pixels:'</span>, image[:<span class="number">10</span>])</div><div class="line"></div><div class="line">        <span class="comment"># We'll show the image and its pixel value histogram side-by-side.</span></div><div class="line">        <span class="comment"># 输出原始图片和直方图，来看看具体的样子</span></div><div class="line">        _, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line"></div><div class="line">        <span class="comment"># To interpret the values as a 28x28 image, we need to reshape</span></div><div class="line">        <span class="comment"># the numpy array, which is one dimensional.</span></div><div class="line">        ax1.imshow(image.reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=plt.cm.Greys)</div><div class="line">        ax2.hist(image, bins=<span class="number">20</span>, range=[<span class="number">0</span>,<span class="number">255</span>])</div><div class="line">        plt.show()</div><div class="line"></div><div class="line">        <span class="comment"># 这里是把 [0, 255] 映射到 [-0.5, 0.5] 之后的展示</span></div><div class="line">        <span class="comment"># Let's convert the uint8 image to 32 bit floats and rescale </span></div><div class="line">        <span class="comment"># the values to be centered around 0, between [-0.5, 0.5]. </span></div><div class="line">        <span class="comment"># </span></div><div class="line">        <span class="comment"># We again plot the image and histogram to check that we </span></div><div class="line">        <span class="comment"># haven't mangled the data.</span></div><div class="line">        scaled = image.astype(numpy.float32)</div><div class="line">        scaled = (scaled - (<span class="number">255</span> / <span class="number">2.0</span>)) / <span class="number">255</span></div><div class="line">        _, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line">        ax1.imshow(scaled.reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=plt.cm.Greys)</div><div class="line">        ax2.hist(scaled, bins=<span class="number">20</span>, range=[<span class="number">-0.5</span>, <span class="number">0.5</span>])</div><div class="line">        plt.show()</div><div class="line"></div><div class="line">        <span class="comment"># 这里读取 Label，也是一个测试而已</span></div><div class="line">    <span class="keyword">with</span> gzip.open(test_labels_filename) <span class="keyword">as</span> f:</div><div class="line">        <span class="comment"># Print the header fields.</span></div><div class="line">        <span class="keyword">for</span> field <span class="keyword">in</span> [<span class="string">'magic number'</span>, <span class="string">'label count'</span>]:</div><div class="line">            print(field, struct.unpack(<span class="string">'&gt;i'</span>, f.read(<span class="number">4</span>))[<span class="number">0</span>])</div><div class="line"></div><div class="line">        print(<span class="string">'First label:'</span>, struct.unpack(<span class="string">'B'</span>, f.read(<span class="number">1</span>))[<span class="number">0</span>]) </div><div class="line"></div><div class="line"><span class="comment"># 简单显示一下，然后进行之后的步骤</span></div><div class="line">sanity_check()</div><div class="line"></div><div class="line"><span class="comment"># 处理图片数据</span></div><div class="line">IMAGE_SIZE = <span class="number">28</span></div><div class="line">PIXEL_DEPTH = <span class="number">255</span></div><div class="line"></div><div class="line"><span class="comment"># 这个函数会提取并处理数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_data</span><span class="params">(filename, num_images)</span>:</span></div><div class="line">    <span class="string">"""Extract the images into a 4D tensor [image index, y, x, channels].</span></div><div class="line">  </div><div class="line">    For MNIST data, the number of channels is always 1.</div><div class="line"></div><div class="line">    Values are rescaled from [0, 255] down to [-0.5, 0.5].</div><div class="line">    """</div><div class="line">    print(<span class="string">'Extracting'</span>, filename)</div><div class="line">    <span class="keyword">with</span> gzip.open(filename) <span class="keyword">as</span> bytestream:</div><div class="line">        <span class="comment"># Skip the magic number and dimensions; we know these values.</span></div><div class="line">        bytestream.read(<span class="number">16</span>)</div><div class="line"></div><div class="line">        buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)</div><div class="line">        data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)</div><div class="line">        data = (data - (PIXEL_DEPTH / <span class="number">2.0</span>)) / PIXEL_DEPTH</div><div class="line">        data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, <span class="number">1</span>)</div><div class="line">        <span class="keyword">return</span> data</div><div class="line"></div><div class="line">train_data = extract_data(train_data_filename, <span class="number">60000</span>)</div><div class="line">test_data = extract_data(test_data_filename, <span class="number">10000</span>)</div><div class="line"></div><div class="line"><span class="comment"># 这里把处理后的输出展示下</span></div><div class="line">print(<span class="string">'Training data shape'</span>, train_data.shape)</div><div class="line">_, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line">ax1.imshow(train_data[<span class="number">0</span>].reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=plt.cm.Greys)</div><div class="line">ax2.imshow(train_data[<span class="number">1</span>].reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=plt.cm.Greys)</div><div class="line">plt.show()</div><div class="line"></div><div class="line"><span class="comment"># 接下来处理标签，我们需要把类别处理成向量，如果是第二类，那么对应 [0,1,0,...,0]，即第二个位置为 1</span></div><div class="line">NUM_LABELS = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_labels</span><span class="params">(filename, num_images)</span>:</span></div><div class="line">    <span class="string">"""Extract the labels into a 1-hot matrix [image index, label index]."""</span></div><div class="line">    print(<span class="string">'Extracting'</span>, filename)</div><div class="line">    <span class="keyword">with</span> gzip.open(filename) <span class="keyword">as</span> bytestream:</div><div class="line">        <span class="comment"># Skip the magic number and count; we know these values.</span></div><div class="line">        bytestream.read(<span class="number">8</span>)</div><div class="line">        buf = bytestream.read(<span class="number">1</span> * num_images)</div><div class="line">        labels = numpy.frombuffer(buf, dtype=numpy.uint8)</div><div class="line">    <span class="comment"># Convert to dense 1-hot representation.</span></div><div class="line">    <span class="keyword">return</span> (numpy.arange(NUM_LABELS) == labels[:, <span class="keyword">None</span>]).astype(numpy.float32)</div><div class="line"></div><div class="line">train_labels = extract_labels(train_labels_filename, <span class="number">60000</span>)</div><div class="line">test_labels = extract_labels(test_labels_filename, <span class="number">10000</span>)</div><div class="line"></div><div class="line"><span class="comment"># 同样测试一下数据</span></div><div class="line">print(<span class="string">'Training labels shape'</span>, train_labels.shape)</div><div class="line">print(<span class="string">'First label vector'</span>, train_labels[<span class="number">0</span>])</div><div class="line">print(<span class="string">'Second label vector'</span>, train_labels[<span class="number">1</span>])</div><div class="line"></div><div class="line"><span class="comment"># 这里我们把数据分成训练、测试和验证集</span></div><div class="line">VALIDATION_SIZE = <span class="number">5000</span></div><div class="line"></div><div class="line">validation_data = train_data[:VALIDATION_SIZE, :, :, :]</div><div class="line">validation_labels = train_labels[:VALIDATION_SIZE]</div><div class="line">train_data = train_data[VALIDATION_SIZE:, :, :, :]</div><div class="line">train_labels = train_labels[VALIDATION_SIZE:]</div><div class="line"></div><div class="line">train_size = train_labels.shape[<span class="number">0</span>]</div><div class="line"></div><div class="line">print(<span class="string">'Validation shape'</span>, validation_data.shape)</div><div class="line">print(<span class="string">'Train size'</span>, train_size)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 这里开始定义模型</span></div><div class="line"><span class="comment"># 从原始输入开始，进行卷积(convolution)和池化(max pooling)处理，在全连接层之前会用 ReLU </span></div><div class="line"><span class="comment"># 作为激活函数，最后用 softmax 来处理输出，把类别信息转化成概率，训练的时候使用 Dropout</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># 准备模型可以分三步</span></div><div class="line"><span class="comment"># 1. 定义变量，来保存我们要训练的权重 weights</span></div><div class="line"><span class="comment"># 2. 定义模型的图结构</span></div><div class="line"><span class="comment"># 3. 把模型的图分别用于训练、测试和验证（复制几份）</span></div><div class="line"></div><div class="line"><span class="comment"># 先处理好变量</span></div><div class="line"><span class="comment"># 从效率的角度考虑，我们会把样本分组，这里是一个组的样本数量</span></div><div class="line">BATCH_SIZE = <span class="number">60</span></div><div class="line"><span class="comment"># 因为是灰度图，所以只有一个通道 channel</span></div><div class="line">NUM_CHANNELS = <span class="number">1</span></div><div class="line"><span class="comment"># 固定随机种子，保证每次的结果一致（不然没办法验证数据和模型）</span></div><div class="line">SEED = <span class="number">42</span></div><div class="line"></div><div class="line"><span class="comment"># 我们在这里把训练数据和类别标签『喂』给模型，不过这里只是一个占位符(placeholder)</span></div><div class="line"><span class="comment"># 真正训练的时候，这些节点在每一步会获取批量数据</span></div><div class="line">train_data_node = tf.placeholder(</div><div class="line">  tf.float32,</div><div class="line">  shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))</div><div class="line">train_labels_node = tf.placeholder(tf.float32,</div><div class="line">                                   shape=(BATCH_SIZE, NUM_LABELS))</div><div class="line"></div><div class="line"><span class="comment"># 对于验证和测试数据，直接保存到一个常量节点里即可（不存在训练的过程，不需要是变量）</span></div><div class="line">validation_data_node = tf.constant(validation_data)</div><div class="line">test_data_node = tf.constant(test_data)</div><div class="line"></div><div class="line"><span class="comment"># 下面的变量保存着所有的需要训练的权重。后面的参数定义了这些变量的初始化条件</span></div><div class="line"><span class="comment"># 用高斯分布初始化卷积的 weights</span></div><div class="line">conv1_weights = tf.Variable(</div><div class="line">  tf.truncated_normal([<span class="number">5</span>, <span class="number">5</span>, NUM_CHANNELS, <span class="number">32</span>],  <span class="comment"># 5x5 filter, depth 32.</span></div><div class="line">                      stddev=<span class="number">0.1</span>,</div><div class="line">                      seed=SEED))</div><div class="line"><span class="comment"># 初始的 bias 为 0</span></div><div class="line">conv1_biases = tf.Variable(tf.zeros([<span class="number">32</span>]))</div><div class="line"><span class="comment"># 第二层的卷积权重，32 个输入（对应上面的 32），然后下面是 64 维</span></div><div class="line">conv2_weights = tf.Variable(</div><div class="line">  tf.truncated_normal([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>],</div><div class="line">                      stddev=<span class="number">0.1</span>,</div><div class="line">                      seed=SEED))</div><div class="line"><span class="comment"># 同理，bias 也是 64 维，但是这里用 0.1</span></div><div class="line">conv2_biases = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">64</span>]))</div><div class="line"><span class="comment"># 然后是一个全连接的网络，共 512 维，为什么呢，因为我们有卷积和池化的存在，所以是 32*64/4</span></div><div class="line"><span class="comment"># (?这里我也不是很确定)</span></div><div class="line">fc1_weights = tf.Variable(  <span class="comment"># fully connected, depth 512.</span></div><div class="line">  tf.truncated_normal([IMAGE_SIZE // <span class="number">4</span> * IMAGE_SIZE // <span class="number">4</span> * <span class="number">64</span>, <span class="number">512</span>],</div><div class="line">                      stddev=<span class="number">0.1</span>,</div><div class="line">                      seed=SEED))</div><div class="line">fc1_biases = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">512</span>]))</div><div class="line">fc2_weights = tf.Variable(</div><div class="line">  tf.truncated_normal([<span class="number">512</span>, NUM_LABELS],</div><div class="line">                      stddev=<span class="number">0.1</span>,</div><div class="line">                      seed=SEED))</div><div class="line">fc2_biases = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[NUM_LABELS]))</div><div class="line"></div><div class="line">print(<span class="string">'变量设置完毕'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 定义好了各种需要训练的变量，我们可以在 TensorFlow 图中把这些变量连起来了</span></div><div class="line"><span class="comment"># 这里我们用一个函数来返回我们需要的 tf graph，这里有一个参数来控制是训练还是其他</span></div><div class="line"><span class="comment"># 如果是训练，我们需要使用 dropout</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(data, train=False)</span>:</span></div><div class="line">    <span class="string">"""模型定义"""</span></div><div class="line">    <span class="comment"># 2D 卷积，使用相同 padding，意思是输入的 feature 大小和输出的一致，</span></div><div class="line">    <span class="comment"># strides 是一个四维数组 [image index, y, x, depth]</span></div><div class="line">    conv = tf.nn.conv2d(data,</div><div class="line">                        conv1_weights,</div><div class="line">                        strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</div><div class="line">                        padding=<span class="string">'SAME'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 对卷积和偏置做 ReLU 操作</span></div><div class="line">    <span class="comment"># Bias and rectified linear non-linearity.</span></div><div class="line">    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))</div><div class="line"></div><div class="line">    <span class="comment"># 池化，这里我们的 pooling window 是 2，每个 stride 是 2</span></div><div class="line">    <span class="comment"># Max pooling. The kernel size spec ksize also follows the layout of</span></div><div class="line">    <span class="comment"># the data. Here we have a pooling window of 2, and a stride of 2.</span></div><div class="line">    pool = tf.nn.max_pool(relu,</div><div class="line">                          ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</div><div class="line">                          strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</div><div class="line">                          padding=<span class="string">'SAME'</span>)</div><div class="line">    conv = tf.nn.conv2d(pool,</div><div class="line">                        conv2_weights,</div><div class="line">                        strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</div><div class="line">                        padding=<span class="string">'SAME'</span>)</div><div class="line">    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))</div><div class="line">    pool = tf.nn.max_pool(relu,</div><div class="line">                          ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</div><div class="line">                          strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</div><div class="line">                          padding=<span class="string">'SAME'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 把 feature map 转为 2D 矩阵，并传给全连接网络</span></div><div class="line">    pool_shape = pool.get_shape().as_list()</div><div class="line">    reshape = tf.reshape(</div><div class="line">        pool,</div><div class="line">        [pool_shape[<span class="number">0</span>], pool_shape[<span class="number">1</span>] * pool_shape[<span class="number">2</span>] * pool_shape[<span class="number">3</span>]])</div><div class="line">  </div><div class="line">    <span class="comment"># Fully connected layer. Note that the '+' operation automatically</span></div><div class="line">    <span class="comment"># broadcasts the biases.</span></div><div class="line">    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)</div><div class="line"></div><div class="line">    <span class="comment"># Add a 50% dropout during training only. Dropout also scales</span></div><div class="line">    <span class="comment"># activations such that no rescaling is needed at evaluation time.</span></div><div class="line">    <span class="keyword">if</span> train:</div><div class="line">        hidden = tf.nn.dropout(hidden, <span class="number">0.5</span>, seed=SEED)</div><div class="line">    <span class="keyword">return</span> tf.matmul(hidden, fc2_weights) + fc2_biases</div><div class="line"></div><div class="line"><span class="comment"># 定义了图的基本结构，我们就可以分别为 训练、测试和验证来提取模型了（也会根据不同的类型做一些自定义）</span></div><div class="line"><span class="comment"># train_prediction 保存训练的图，使用 cross-entropy loss 和 weight regularization</span></div><div class="line"><span class="comment"># 我们也会在训练的过程中调整学习率（通过 exponential_decay 操作来完成，会使用 MomentumOptimizer）</span></div><div class="line"></div><div class="line"><span class="comment"># 验证和测试的图比较简单，我们只需要使用验证和测试集作为输入，用 softmax 分类器作为输出</span></div><div class="line"></div><div class="line"><span class="comment"># 训练的计算</span></div><div class="line"><span class="comment"># Training computation: logits + cross-entropy loss.</span></div><div class="line">logits = model(train_data_node, <span class="keyword">True</span>)</div><div class="line">loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(</div><div class="line">  labels=train_labels_node, logits=logits))</div><div class="line"></div><div class="line"><span class="comment"># L2 正则化</span></div><div class="line"><span class="comment"># L2 regularization for the fully connected parameters.</span></div><div class="line">regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +</div><div class="line">                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))</div><div class="line"><span class="comment"># Add the regularization term to the loss.</span></div><div class="line">loss += <span class="number">5e-4</span> * regularizers</div><div class="line"></div><div class="line"><span class="comment"># Optimizer: set up a variable that's incremented once per batch and</span></div><div class="line"><span class="comment"># controls the learning rate decay.</span></div><div class="line">batch = tf.Variable(<span class="number">0</span>)</div><div class="line"><span class="comment"># Decay once per epoch, using an exponential schedule starting at 0.01.</span></div><div class="line">learning_rate = tf.train.exponential_decay(</div><div class="line">  <span class="number">0.01</span>,                <span class="comment"># Base learning rate.</span></div><div class="line">  batch * BATCH_SIZE,  <span class="comment"># Current index into the dataset.</span></div><div class="line">  train_size,          <span class="comment"># Decay step.</span></div><div class="line">  <span class="number">0.95</span>,                <span class="comment"># Decay rate.</span></div><div class="line">  staircase=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># Use simple momentum for the optimization.</span></div><div class="line">optimizer = tf.train.MomentumOptimizer(learning_rate,</div><div class="line">                                       <span class="number">0.9</span>).minimize(loss,</div><div class="line">                                                     global_step=batch)</div><div class="line"></div><div class="line"><span class="comment"># Predictions for the minibatch, validation set and test set.</span></div><div class="line">train_prediction = tf.nn.softmax(logits)</div><div class="line"><span class="comment"># We'll compute them only once in a while by calling their &#123;eval()&#125; method.</span></div><div class="line">validation_prediction = tf.nn.softmax(model(validation_data_node))</div><div class="line">test_prediction = tf.nn.softmax(model(test_data_node))</div><div class="line"></div><div class="line"><span class="comment"># 准备好了训练、测试和验证的模型之后，我们就可以来真正执行训练了。</span></div><div class="line"><span class="comment"># 所有的操作都需要在 session 中，在 python 中像是</span></div><div class="line"><span class="comment"># with tf.Session() as s:</span></div><div class="line"><span class="comment"># ...training / test / evaluation loop...</span></div><div class="line"></div><div class="line"><span class="comment"># 但是我们这里想要保持 session 方便我们去探索训练的过程，使用 InteractiveSession</span></div><div class="line"></div><div class="line"><span class="comment"># 我们先创建一个 session 并初始化我们刚才定义的变量</span></div><div class="line">s = tf.InteractiveSession()</div><div class="line"></div><div class="line"><span class="comment"># Use our newly created session as the default for subsequent operations.</span></div><div class="line">s.as_default()</div><div class="line"></div><div class="line"><span class="comment"># 初始化刚才定义的变量</span></div><div class="line">tf.global_variables_initializer().run()</div><div class="line"></div><div class="line"><span class="comment"># 我们现在可以开始训练了，这里我们用 minibatch 的方法（而不是一次只训练一个样本）</span></div><div class="line">BATCH_SIZE = <span class="number">60</span></div><div class="line"></div><div class="line"><span class="comment"># 提取第一个 batch 的数据和标签</span></div><div class="line"><span class="comment"># Grab the first BATCH_SIZE examples and labels.</span></div><div class="line">batch_data = train_data[:BATCH_SIZE, :, :, :]</div><div class="line">batch_labels = train_labels[:BATCH_SIZE]</div><div class="line"></div><div class="line"><span class="comment"># This dictionary maps the batch data (as a numpy array) to the</span></div><div class="line"><span class="comment"># node in the graph it should be fed to.</span></div><div class="line">feed_dict = &#123;train_data_node: batch_data,</div><div class="line">             train_labels_node: batch_labels&#125;</div><div class="line"></div><div class="line"><span class="comment"># Run the graph and fetch some of the nodes.</span></div><div class="line">_, l, lr, predictions = s.run(</div><div class="line">  [optimizer, loss, learning_rate, train_prediction],</div><div class="line">  feed_dict=feed_dict)</div><div class="line"></div><div class="line">print(predictions[<span class="number">0</span>])</div><div class="line"></div><div class="line"><span class="comment"># The highest probability in the first entry.</span></div><div class="line">print(<span class="string">'First prediction'</span>, numpy.argmax(predictions[<span class="number">0</span>]))</div><div class="line"></div><div class="line"><span class="comment"># But, predictions is actually a list of BATCH_SIZE probability vectors.</span></div><div class="line">print(predictions.shape)</div><div class="line"></div><div class="line"><span class="comment"># So, we'll take the highest probability for each vector.</span></div><div class="line">print(<span class="string">'All predictions'</span>, numpy.argmax(predictions, <span class="number">1</span>))</div><div class="line"></div><div class="line">print(<span class="string">'Batch labels'</span>, numpy.argmax(batch_labels, <span class="number">1</span>))</div><div class="line"></div><div class="line">correct = numpy.sum(numpy.argmax(predictions, <span class="number">1</span>) == numpy.argmax(batch_labels, <span class="number">1</span>))</div><div class="line">total = predictions.shape[<span class="number">0</span>]</div><div class="line"></div><div class="line">print(float(correct) / float(total))</div><div class="line"></div><div class="line">confusions = numpy.zeros([<span class="number">10</span>, <span class="number">10</span>], numpy.float32)</div><div class="line">bundled = zip(numpy.argmax(predictions, <span class="number">1</span>), numpy.argmax(batch_labels, <span class="number">1</span>))</div><div class="line"><span class="keyword">for</span> predicted, actual <span class="keyword">in</span> bundled:</div><div class="line">  confusions[predicted, actual] += <span class="number">1</span></div><div class="line"></div><div class="line">plt.grid(<span class="keyword">False</span>)</div><div class="line">plt.xticks(numpy.arange(NUM_LABELS))</div><div class="line">plt.yticks(numpy.arange(NUM_LABELS))</div><div class="line">plt.imshow(confusions, cmap=plt.cm.jet, interpolation=<span class="string">'nearest'</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">error_rate</span><span class="params">(predictions, labels)</span>:</span></div><div class="line">    <span class="string">"""Return the error rate and confusions."""</span></div><div class="line">    correct = numpy.sum(numpy.argmax(predictions, <span class="number">1</span>) == numpy.argmax(labels, <span class="number">1</span>))</div><div class="line">    total = predictions.shape[<span class="number">0</span>]</div><div class="line"></div><div class="line">    error = <span class="number">100.0</span> - (<span class="number">100</span> * float(correct) / float(total))</div><div class="line"></div><div class="line">    confusions = numpy.zeros([<span class="number">10</span>, <span class="number">10</span>], numpy.float32)</div><div class="line">    bundled = zip(numpy.argmax(predictions, <span class="number">1</span>), numpy.argmax(labels, <span class="number">1</span>))</div><div class="line">    <span class="keyword">for</span> predicted, actual <span class="keyword">in</span> bundled:</div><div class="line">        confusions[predicted, actual] += <span class="number">1</span></div><div class="line">    </div><div class="line">    <span class="keyword">return</span> error, confusions</div><div class="line"></div><div class="line"><span class="comment"># 这里训练 n 轮，每轮都是 minibatch</span></div><div class="line">train_round = <span class="number">3</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(train_round):</div><div class="line">    print(<span class="string">"Training Round "</span>, i+<span class="number">1</span> )</div><div class="line">    <span class="comment"># Train over the first 1/4th of our training set.</span></div><div class="line">    steps = train_size // BATCH_SIZE</div><div class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(steps):</div><div class="line">        <span class="comment"># Compute the offset of the current minibatch in the data.</span></div><div class="line">        <span class="comment"># Note that we could use better randomization across epochs.</span></div><div class="line">        offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)</div><div class="line">        batch_data = train_data[offset:(offset + BATCH_SIZE), :, :, :]</div><div class="line">        batch_labels = train_labels[offset:(offset + BATCH_SIZE)]</div><div class="line">        <span class="comment"># This dictionary maps the batch data (as a numpy array) to the</span></div><div class="line">        <span class="comment"># node in the graph it should be fed to.</span></div><div class="line">        feed_dict = &#123;train_data_node: batch_data,</div><div class="line">                    train_labels_node: batch_labels&#125;</div><div class="line">        <span class="comment"># Run the graph and fetch some of the nodes.</span></div><div class="line">        _, l, lr, predictions = s.run(</div><div class="line">        [optimizer, loss, learning_rate, train_prediction],</div><div class="line">        feed_dict=feed_dict)</div><div class="line">        </div><div class="line">        <span class="comment"># Print out the loss periodically.</span></div><div class="line">        <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">            error, _ = error_rate(predictions, batch_labels)</div><div class="line">            print(<span class="string">'Step %d of %d'</span> % (step, steps))</div><div class="line">            print(<span class="string">'Mini-batch loss: %.5f Error: %.5f Learning rate: %.5f'</span> % (l, error, lr))</div><div class="line">            print(<span class="string">'Validation error: %.1f%%'</span> % error_rate(</div><div class="line">                validation_prediction.eval(), validation_labels)[<span class="number">0</span>])</div><div class="line"></div><div class="line">test_error, confusions = error_rate(test_prediction.eval(), test_labels)</div><div class="line">print(<span class="string">'Test error: %.1f%%'</span> % test_error)</div><div class="line"></div><div class="line">plt.xlabel(<span class="string">'Actual'</span>)</div><div class="line">plt.ylabel(<span class="string">'Predicted'</span>)</div><div class="line">plt.grid(<span class="keyword">False</span>)</div><div class="line">plt.xticks(numpy.arange(NUM_LABELS))</div><div class="line">plt.yticks(numpy.arange(NUM_LABELS))</div><div class="line">plt.imshow(confusions, cmap=plt.cm.jet, interpolation=<span class="string">'nearest'</span>);</div><div class="line"></div><div class="line"><span class="keyword">for</span> i, cas <span class="keyword">in</span> enumerate(confusions):</div><div class="line">    <span class="keyword">for</span> j, count <span class="keyword">in</span> enumerate(cas):</div><div class="line">        <span class="keyword">if</span> count &gt; <span class="number">0</span>:</div><div class="line">            xoff = <span class="number">.07</span> * len(str(count))</div><div class="line">            plt.text(j-xoff, i+<span class="number">.2</span>, int(count), fontsize=<span class="number">9</span>, color=<span class="string">'white'</span>)</div><div class="line">plt.show()</div><div class="line"></div><div class="line">plt.xticks(numpy.arange(NUM_LABELS))</div><div class="line">plt.hist(numpy.argmax(test_labels, <span class="number">1</span>))</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<h2 id="官方文档阅读笔记"><a href="#官方文档阅读笔记" class="headerlink" title="官方文档阅读笔记"></a>官方文档阅读笔记</h2><p>使用 TensorFlow，你必须明白 TensorFlow</p>
<ul>
<li>使用<strong>图</strong>(graph)来表示计算任务</li>
<li>在被称之为<strong>会话</strong>(Session)的<strong>上下文</strong>(context)中执行图</li>
<li>使用 tensor 表示数据</li>
<li>通过<strong>变量</strong>(Variable)维护状态</li>
<li>使用 feed 和 fetch 可以为<strong>任意的操作</strong>(arbitrary operation)赋值或者从其中获取数据</li>
</ul>
<p>TensorFlow 是一个编程系统，使用图来表示计算任务。图中的节点被称之为 op (operation 的缩写)。一个 op 获得 0 个或多个 Tensor，执行计算，产生 0 个或多个 Tensor。每个 Tensor 是一个类型化的多维数组。例如，你可以将一小组图像集表示为一个四维浮点数数组，这四个维度分别是 <code>[batch, height, width, channels]</code>。</p>
<p>一个 TensorFlow 图描述了计算的过程。为了进行计算，图必须在<strong>会话</strong>里被启动。<strong>会话</strong>将图的 op 分发到诸如 CPU 或 GPU 之类的设备上，同时提供执行 op 的方法。这些方法执行后，将产生的 tensor 返回。在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象；在 C 和 C++ 语言中，返回的 tensor 是 <code>tensorflow::Tensor</code> 实例。</p>
<ul>
<li>计算图<ul>
<li>TensorFlow 程序通常被组织成一个构建阶段和一个执行阶段。在构建阶段，op 的执行步骤被描述成一个图。在执行阶段，使用会话执行执行图中的 op。例如，通常在构建阶段创建一个图来表示和训练神经网络，然后在执行阶段反复执行图中的训练 op。</li>
<li>TensorFlow 支持 C, C++, Python 编程语言。目前，TensorFlow 的 Python 库更加易用，它提供了大量的辅助函数来简化构建图的工作，这些函数尚未被 C 和 C++ 库支持.</li>
<li>三种语言的会话库 (session libraries) 是一致的。</li>
</ul>
</li>
<li>构建图<ul>
<li>构建图的第一步，是创建源 op (source op)。源 op 不需要任何输入，例如<strong>常量</strong>(Constant)。源 op 的输出被传递给其它 op 做运算</li>
<li>Python 库中，op 构造器的返回值代表被构造出的 op 的输出，这些返回值可以传递给其它 op 构造器作为输入</li>
<li>TensorFlow Python 库有一个默认图 (default graph), op 构造器可以为其增加节点。这个默认图对许多程序来说已经足够用了</li>
</ul>
</li>
</ul>
<p>在实现上，TensorFlow 将图形定义转换成分布式执行的操作，以充分利用可用的计算资源(如 CPU 或 GPU). 一般你不需要显式指定使用 CPU 还是 GPU，TensorFlow 能自动检测。如果检测到 GPU，TensorFlow 会尽可能地利用找到的第一个 GPU 来执行操作。</p>
<p>如果机器上有超过一个可用的 GPU，除第一个外的其它 GPU 默认是不参与计算的。为了让 TensorFlow 使用这些 GPU，你必须将 op 明确指派给它们执行。<code>with...Device</code> 语句用来指派特定的 CPU 或 GPU 执行操作:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  <span class="keyword">with</span> tf.device(<span class="string">"/gpu:1"</span>):</div><div class="line">    matrix1 = tf.constant([[<span class="number">3.</span>, <span class="number">3.</span>]])</div><div class="line">    matrix2 = tf.constant([[<span class="number">2.</span>],[<span class="number">2.</span>]])</div><div class="line">    product = tf.matmul(matrix1, matrix2)</div><div class="line">    ...</div></pre></td></tr></table></figure>
<p>设备用字符串进行标识。目前支持的设备包括:</p>
<ul>
<li>“/cpu:0”: 机器的 CPU</li>
<li>“/gpu:0”: 机器的第一个 GPU，如果有的话</li>
<li>“/gpu:1”: 机器的第二个 GPU，以此类推</li>
</ul>
<p>具体可见 <a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/using_gpu.html" target="_blank" rel="external">使用 GPUs</a></p>
<p>文档的其他部分涉及到具体的算法细节，这里不再深究。</p>
<h2 id="GPU-相关"><a href="#GPU-相关" class="headerlink" title="GPU 相关"></a>GPU 相关</h2><ul>
<li>最合适测试 CUDA 的代码是使用 <code>log_device_placement</code> 参数<ul>
<li><code>sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))</code></li>
<li><code>print sess.run(c)</code></li>
<li>观察日志输出是否有 gpu0, gpu1</li>
</ul>
</li>
<li>检测 GPU 的使用率 <code>nvidia-smi -q -g 0 -d UTILIZATION -1</code></li>
<li><a href="https://github.com/NVIDIA/nvidia-docker" target="_blank" rel="external">Nvidia Docker</a><ul>
<li>TODO 需要看如何配置 docker 所能分配的 gpu</li>
</ul>
</li>
<li>tensorflow 在训练时默认占用所有 GPU 的显存，配置的方式有<ul>
<li>构造 <code>tf.Session()</code> 时配置参数（这里是按照百分比来选择），如<ul>
<li><code>gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)</code></li>
<li><code>sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))</code></li>
<li><code>per_process_gpu_memory_fraction</code> 指定每个 GPU 进程中使用显存的上限，但只能作用于所有 GPU，不能根据不同 GPU 单独配置</li>
</ul>
</li>
<li>设置显存根据需求增长<ul>
<li><code>config = tf.ConfigProto()</code></li>
<li><code>config.gpu_options.allow_growth=True</code></li>
<li><code>sess = tf.Session(config=config)</code></li>
</ul>
</li>
<li>在执行训练脚本前使用 <code>export CUDA_VISIBLE_DEVICES=1</code> 来限制可见的 GPU 数目，如果是 python 脚本，可以用 <code>CUDA_VISIBLE_DEVICES=1 python my_script.py</code>，如果想用两个卡，则是 <code>CUDA_VISIBLE_DEVICES=0,1</code>，如果禁用 GPU，则是 <code>CUDA_VISIBLE_DEVICES=&quot;&quot;</code><ul>
<li>可以参考 <a href="https://stackoverflow.com/questions/34775522/tensorflow-mutiple-sessions-with-mutiple-gpus" target="_blank" rel="external">Tensorflow multiple sessions with multiple GPUs</a></li>
</ul>
</li>
<li>也可以在 Python 代码中进行设置<ul>
<li><code>import os</code></li>
<li><code>os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0&quot;</code></li>
</ul>
</li>
<li>如果需要同时限制显存大小，也按需增长，那么可以这样<ul>
<li><code>os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &#39;0&#39;</code> 指定第一块 GPU 可用</li>
<li><code>config = tf.ConfigProto()</code></li>
<li><code>config.gpu_options.per_process_gpu_memory_fraction = 0.5</code> 最多只能占用指定 gpu 50% 显存</li>
<li><code>config.gpu_options.allow_growth = True</code> 程序按需申请内存</li>
<li><code>sess = tf.Session(config = config)</code></li>
</ul>
</li>
<li>需要注意的是，虽然代码或配置层面设置了对显存占用百分比阈值，但实际中如果达到了，程序有需要的话还是会突破的，以上的显存限制仅仅为了跑小数据集时避免对显存的浪费</li>
</ul>
</li>
</ul>
<h2 id="Nvidia-Docker"><a href="#Nvidia-Docker" class="headerlink" title="Nvidia Docker"></a>Nvidia Docker</h2><ul>
<li><p><a href="https://github.com/NVIDIA/nvidia-docker" target="_blank" rel="external">Github</a></p>
</li>
<li><p>在 Nvidia Docker 层限制 GPU 资源</p>
<ul>
<li><code>NV_GPU=0,1 nvidia-docker run -it nvidia/cuda nvidia-smi</code></li>
<li>弄清楚 <code>nvidia_uvm</code> 是个啥</li>
<li><code>nvidia-docker</code> 相当于 <code>docker run --device=/dev/nvidiactl --device=/dev/nvidia-utm --device=/dev/nvidia0</code></li>
</ul>
</li>
<li>可以做到资源隔离，具体要测试下，看看提供了什么工具</li>
</ul>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://github.com/wdxtub/Machine-Learning-Project" target="_blank" rel="external">项目源代码</a></li>
<li><a href="https://github.com/jikexueyuanwiki/tensorflow-zh" target="_blank" rel="external">TensorFlow 官方文档中文版（比较老）</a></li>
<li><a href="https://github.com/jtoy/awesome-tensorflow" target="_blank" rel="external">Awesome TensorFlow</a></li>
<li><a href="https://github.com/fluxcapacitor/pipeline" target="_blank" rel="external">PipelineIO - 收费</a></li>
<li><a href="https://github.com/polyaxon/polyaxon" target="_blank" rel="external">Polyaxon</a></li>
<li><a href="http://www.cnblogs.com/darkknightzh/p/6591923.html" target="_blank" rel="external">http://www.cnblogs.com/darkknightzh/p/6591923.html</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/23250782" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/23250782</a></li>
<li><a href="http://www.cnblogs.com/wangxiaocvpr/p/5902086.html" target="_blank" rel="external">http://www.cnblogs.com/wangxiaocvpr/p/5902086.html</a></li>
<li><a href="http://blog.csdn.net/luodongri/article/details/52596780" target="_blank" rel="external">http://blog.csdn.net/luodongri/article/details/52596780</a></li>
</ul>
<h2 id="广告时间"><a href="#广告时间" class="headerlink" title="广告时间"></a>广告时间</h2><p>优秀人才不缺工作机会，只缺适合自己的好机会。但是他们往往没有精力从海量机会中找到最适合的那个。100offer 会对平台上的人才和企业进行严格筛选，让「最好的人才」和「最好的公司」相遇。</p>
<p><a href="https://cn.100offer.com/how-it-works/?utm_source=wdxtub&amp;utm_medium=display&amp;utm_campaign=wdxtub_20170703&amp;utm_content=find_new_job&amp;campaign_code=wdxtub" target="_blank" rel="external">点击注册 100offer</a>，谈谈你对下一份工作的期待，收获 5-10 个满足你要求的好机会。</p>
 <a href="https://cn.100offer.com/how-it-works/?utm_source=wdxtub&utm_medium=display&utm_campaign=wdxtub_20170703&utm_content=find_new_job&campaign_code=wdxtub" target="_blank" rel="external"> <img src="/misc/100offer.jpg" alt=""> </a>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>捧个钱场？</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/misc/wechat.jpg" alt="wdxtub WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="/misc/alipay.jpg" alt="wdxtub Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
            <a href="/tags/TensorFlow/" rel="tag"># TensorFlow</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/05/31/structure-of-science-revolution-clip/" rel="next" title="【科学革命的结构】读书笔记">
                <i class="fa fa-chevron-left"></i> 【科学革命的结构】读书笔记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/06/02/let-it-be/" rel="prev" title="第五十一周 - 随缘">
                第五十一周 - 随缘 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
  <a class="jiathis_button_tsina"></a>
  <a class="jiathis_button_tqq"></a>
  <a class="jiathis_button_weixin"></a>
  <a class="jiathis_button_cqq"></a>
  <a class="jiathis_button_douban"></a>
  <a class="jiathis_button_renren"></a>
  <a class="jiathis_button_qzone"></a>
  <a class="jiathis_button_kaixin001"></a>
  <a class="jiathis_button_copy"></a>
  <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a>
  <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
  var jiathis_config={
    hideMore:false
  }
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END -->

      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/misc/avatar.jpg"
               alt="wdxtub" />
          <p class="site-author-name" itemprop="name">wdxtub</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">865</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">956</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/wdxtub" target="_blank" title="GitHub">
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/wdxtub" target="_blank" title="微博">
                  
                  微博
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://douban.com/people/wdx" target="_blank" title="豆瓣">
                  
                  豆瓣
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/wdxtub" target="_blank" title="知乎">
                  
                  知乎
                </a>
              </span>
            
          
        </div>

        
        
          <div class="cc-license motion-element" itemprop="license">
            <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" class="cc-opacity" target="_blank">
              <img src="/images/cc-by-nc-nd.svg" alt="Creative Commons" />
            </a>
          </div>
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              不妨看看
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://zhchbin.github.io/" title="zhchbin" target="_blank">zhchbin</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.algorithmdog.com/" title="算法狗" target="_blank">算法狗</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.52cs.org/" title="我爱计算机" target="_blank">我爱计算机</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://wdxtub.lofter.com/" title="我的 Lofter" target="_blank">我的 Lofter</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://wdxtub.com/interview/" title="刷题笔记" target="_blank">刷题笔记</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#快速入门"><span class="nav-number">1.</span> <span class="nav-text">快速入门</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#一个简单的神经网络"><span class="nav-number">2.</span> <span class="nav-text">一个简单的神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#从零开始上手-MNIST-数据集"><span class="nav-number">3.</span> <span class="nav-text">从零开始上手 MNIST 数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#官方文档阅读笔记"><span class="nav-number">4.</span> <span class="nav-text">官方文档阅读笔记</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GPU-相关"><span class="nav-number">5.</span> <span class="nav-text">GPU 相关</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Nvidia-Docker"><span class="nav-number">6.</span> <span class="nav-text">Nvidia Docker</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考链接"><span class="nav-number">7.</span> <span class="nav-text">参考链接</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#广告时间"><span class="nav-number">8.</span> <span class="nav-text">广告时间</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2013 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wdxtub</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  
    
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "617c94df1165440eaa2e0f239c18d092",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
  







  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  


  

</body>
</html>
